import torch
import torch.nn as nn
import torch.nn.functional as F

# Enhanced Token Mixing Block
class EnhancedTokenMixing(nn.Module):
    def __init__(self, num_patches, token_dim):
        super(EnhancedTokenMixing, self).__init__()
        self.depthwise_conv = nn.Conv1d(num_patches, num_patches, kernel_size=3, padding=1, groups=num_patches)
        self.batch_norm = nn.BatchNorm1d(num_patches)
        self.fc = nn.Linear(num_patches, token_dim)
    
    def forward(self, x):
        # Transpose to shape (B, num_patches, dim) -> (B, dim, num_patches)
        x = x.transpose(1, 2)  
        x = self.depthwise_conv(x)  # Apply depthwise convolution
        x = self.batch_norm(x)  # Apply batch normalization
        x = F.gelu(x)  # GELU activation
        x = self.fc(x.transpose(1, 2))  # Fully connected layer, transpose back to (B, num_patches, token_dim)
        return x

# Channel Mixing Block
class ChannelMixing(nn.Module):
    def __init__(self, dim, channel_dim):
        super(ChannelMixing, self).__init__()
        self.fc1 = nn.Linear(dim, channel_dim)
        self.fc2 = nn.Linear(channel_dim, dim)
    
    def forward(self, x):
        x = F.gelu(self.fc1(x))  # GELU activation
        x = self.fc2(x)  # Fully connected layer
        return x

# MLP Mixer Class
class MLPMixer(nn.Module):
    def __init__(self, image_size, patch_size, num_classes, dim, token_dim, channel_dim):
        super(MLPMixer, self).__init__()
        self.num_patches = (image_size // patch_size) ** 2
        self.patch_size = patch_size
        self.proj = nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size)

        # Create the mixing blocks
        self.token_mixing_blocks = nn.ModuleList([EnhancedTokenMixing(self.num_patches, token_dim) for _ in range(4)])  # 4 Token Mixing Blocks
        self.channel_mixing_blocks = nn.ModuleList([ChannelMixing(dim, channel_dim) for _ in range(4)])  # 4 Channel Mixing Blocks

        self.classifier = nn.Linear(dim, num_classes)

    def forward(self, x):
        # Split the image into patches
        x = self.proj(x).flatten(2).transpose(1, 2)  # Shape: (B, num_patches, dim)

        # Apply token and channel mixing blocks
        for i in range(4):  # For 4 token and channel mixing blocks
            x = x + self.token_mixing_blocks[i](x)  # Token mixing
            x = x + self.channel_mixing_blocks[i](x)  # Channel mixing

        x = x.mean(dim=1)  # Global average pooling
        return self.classifier(x)

# Hyperparameters
image_size = 224
patch_size = 16
num_classes = 3
dim = 512
token_dim = 256
channel_dim = 2048

# Create the model
model = MLPMixer(image_size, patch_size, num_classes, dim, token_dim, channel_dim)


